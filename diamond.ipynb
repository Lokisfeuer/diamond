{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a notbeook as beatiful as a diamond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import jupyter\n",
    "\n",
    "# Don't forget to normalize the dataset - hot encoding\n",
    "# scalars stay normal columns, categories get different columns\n",
    "\n",
    "# ln(price) / invert\n",
    "\n",
    "# max / min normalization the dataset\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def short_roberta(sentences):\n",
    "    sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# roberta fully copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def as_book():\n",
    "    def vectorize_sequences(sequences, dimension=10000):\n",
    "        results = np.zeros((len(sequences), dimension))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            results[i, sequence] = 1.\n",
    "        return results\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)\n",
    "    x_train = vectorize_sequences(train_data)\n",
    "    y_test = vectorize_sequences(test_data)\n",
    "    return x_train, y_test\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    def decode(data, reverse_word_index):\n",
    "        decoded_train_data = []\n",
    "        for review in data:\n",
    "            decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
    "            decoded_train_data.append(decoded_review)\n",
    "        return decoded_train_data\n",
    "\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=100)\n",
    "    train_data = train_data[:int(round(len(train_data) / 2000))]\n",
    "    train_labels = train_labels[:int(round(len(train_labels) / 2000))]\n",
    "    test_data = test_data[:int(round(len(test_data) / 2000))]\n",
    "    test_labels = test_labels[:int(round(len(test_labels) / 2000))]\n",
    "\n",
    "    print(decode(train_data, reverse_word_index)[0])\n",
    "    x_train = long_roberta(decode(train_data, reverse_word_index)).numpy()\n",
    "    x_test = long_roberta(decode(test_data, reverse_word_index)).numpy()\n",
    "    y_train = np.asarray(train_labels).astype('float32')\n",
    "    y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "    # return sentences\n",
    "\n",
    "\n",
    "def main():\n",
    "    x_train, x_test, y_train, y_test = get_data()\n",
    "    '''\n",
    "    x_train = x_train[:len(x_train)/20]\n",
    "    x_test = x_test[:len(x_test)/20]\n",
    "    y_train = y_train[:len(y_train)/20]\n",
    "    y_test = y_test[:len(y_test)/20]\n",
    "    '''\n",
    "    print(type(x_train))\n",
    "    print(len(x_train))\n",
    "    print(type(x_train[0]))\n",
    "    print(len(x_train[0]))\n",
    "    print(x_train[0])\n",
    "\n",
    "    x_val = x_train[:500]\n",
    "    partial_x_train = x_train[500:]\n",
    "    y_val = y_train[:500]\n",
    "    partial_y_train = y_train[500:]\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape='1024'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print('start compiling')\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=256, validation_data=(x_val, y_val))\n",
    "\n",
    "    plot(history)\n",
    "\n",
    "\n",
    "def plot(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict[\"loss\"]\n",
    "    val_loss_values = history_dict[\"val_loss\"]\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import jupyter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class CustomDiamondDataset(Dataset):\n",
    "    def __init__(self, data, prices):\n",
    "        self.x = data\n",
    "        self.y = prices\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('diamonds.csv')\n",
    "    data, prices = normalize_data(data)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "    dataset = CustomDiamondDataset(data, prices)\n",
    "    print(len(data[0]))\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=40, shuffle=True, num_workers=2)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    print(type(data))\n",
    "    print(type(prices))\n",
    "    loss = nn.MSELoss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(100):\n",
    "        for step, (input, label) in enumerate(dataloader):\n",
    "            y_pred = model(input)\n",
    "            l = loss(label, y_pred)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    '''\n",
    "    n_samples, n_features = data.shape\n",
    "    input_size = n_features\n",
    "    model = nn.Linear(input_size, 1) # correct this !\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "    loss = nn.MSELoss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for i in range(100):\n",
    "        y_pred = model(X)\n",
    "        l = loss(Y, y_pred)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # output = model(input).item()\n",
    "    # print(type(data))\n",
    "    # print(prices)\n",
    "    # dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "    '''\n",
    "\n",
    "\n",
    "# both robertas fully copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "def short_roberta(sentences):\n",
    "    sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def invert_min_max(val, min, max):\n",
    "    return (max-min) * val + min\n",
    "\n",
    "def normalize_data(data):\n",
    "    # don't forget to normalize data - hot encoding\n",
    "        # max / min normalization the dataset\n",
    "    # scalars stay normal columns, categories get different columns\n",
    "    # put price on a logarythmic scale\n",
    "\n",
    "    '''\n",
    "    lets build a tensor with the following dimensions:\n",
    "        carat\n",
    "        *cut* (hot encoding)\n",
    "            ideal\n",
    "            premium\n",
    "            good\n",
    "            very good\n",
    "            fair\n",
    "        colour (auch hot encoding)\n",
    "        clarity (dito)\n",
    "        depth\n",
    "        table\n",
    "        price\n",
    "        x\n",
    "        y\n",
    "        z\n",
    "    then min max the full thing.\n",
    "    '''\n",
    "    def onehot():\n",
    "        nb_classes = 6\n",
    "        arr = np.array([[2, 3, 4, 0]])\n",
    "        targets = arr.reshape(-1)\n",
    "        one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        return one_hot_targets\n",
    "\n",
    "    onehot()\n",
    "    np_data = data.to_numpy()\n",
    "\n",
    "    cut_index = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\n",
    "    colour_index = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D':6}\n",
    "    # clarity: (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best))\n",
    "    clarity_index = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1':6, 'IF': 7}\n",
    "    indeces = [cut_index, colour_index, clarity_index]\n",
    "\n",
    "\n",
    "    # carat, cut (5), colour (7), clarity (8), depth, table, price, x, y\n",
    "    new_array = []\n",
    "    prices = []\n",
    "    for i, diamond in enumerate(np_data):\n",
    "        diamond = diamond[1:]\n",
    "        new_diamond = [diamond[0]]\n",
    "        for j in range(3):\n",
    "            index = indeces[j][diamond[j+1]]\n",
    "            zeros = [0]*len(indeces[j].keys())\n",
    "            zeros[index] = 1\n",
    "            for k in zeros:\n",
    "                new_diamond.append(k)\n",
    "        for j in [4, 5, 7, 8]:\n",
    "            new_diamond.append(diamond[j])\n",
    "        new_array.append(new_diamond)\n",
    "        prices.append(math.log(diamond[6]))\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(new_array)\n",
    "    prices = pd.DataFrame(prices)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    return normalized_data, normalized_prices\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
