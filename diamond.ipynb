{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "To test single models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    # test if this works with truncation=False\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "def try_model(model):\n",
    "    a = input('Please enter your input sentence: ')\n",
    "    a = long_roberta(a)\n",
    "    pred = model(a)\n",
    "    print(pred.item())\n",
    "    print('Where 1 is about the topic and 0 is not.\\n')\n",
    "\n",
    "\n",
    "biology = torch.load('model_biology.pt')\n",
    "chemistry = torch.load('model_chemistry.pt')\n",
    "while True:\n",
    "    print('BIOLOGY:')\n",
    "    try_model(biology)\n",
    "    print('CHEMISTRY:')\n",
    "    try_model(chemistry)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the topic identification code - working fine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchmetrics import R2Score\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime as d\n",
    "import math\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "class CustomTopicDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.x = sentences\n",
    "        self.y = labels\n",
    "        self.length = self.x.shape[0]\n",
    "        self.shape = self.x[0].shape[0]\n",
    "        self.feature_names = ['sentences', 'labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class History():\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_'+i: []})\n",
    "            self.history.update({'tra_'+i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "\n",
    "    def save(self, step):\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_'+i: []})\n",
    "            short_history.update({'tra_'+i: []})\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():\n",
    "                    if len(val_pred) > 1:\n",
    "                        val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                        tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                        short_history['val_'+j].append(val_l)\n",
    "                        short_history['tra_'+j].append(tra_l)\n",
    "                self.model.train()\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)\n",
    "\n",
    "\n",
    "    def plot(self, path=None):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            ax.set_title(i.upper())\n",
    "            ax.set_ylabel(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            figures.append(fig)\n",
    "            if path is None:\n",
    "                plt.show()\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f\"{path}/{i}\")\n",
    "        return figures  # what is this?\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(topic, data=None, nr=15):\n",
    "    def ask_ai(nr, prompt):\n",
    "        response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=1, max_tokens=10*nr)\n",
    "        response = '1.' + response['choices'][0]['text'] + '\\n'\n",
    "        l = []\n",
    "        for i in range(nr):\n",
    "            pos = response.find(str(i + 1))\n",
    "            beg = pos + len(str(i + 1)) + 2\n",
    "            end = response[beg:].find('\\n')\n",
    "            l.append(response[beg:beg + end])\n",
    "        return l\n",
    "\n",
    "    def gen_sentences(nr, factor, prompt):\n",
    "        keywords = ask_ai(nr, prompt)\n",
    "        sentences = []\n",
    "        for i in keywords:\n",
    "            print(i)\n",
    "            requests = ask_ai(15*factor, f'Give me {15*factor} independent short requests about \"{i}\".\\n\\n1.')\n",
    "            demands = ask_ai(15*factor, f'Give me {15*factor} independent short demands about \"{i}\".\\n\\n1.')\n",
    "            questions = ask_ai(15*factor, f'Give me {15*factor} independent short questions about \"{i}\".\\n\\n1.')\n",
    "            facts = ask_ai(5*factor, f'Give me {5*factor} independent short factual statements about \"{i}\".\\n\\n1.')\n",
    "            sentences.extend(requests + demands + questions + facts)\n",
    "        return sentences\n",
    "\n",
    "    if data is None:\n",
    "        all_sentences = []\n",
    "        print(f'Writing sentences about {topic}.')\n",
    "        # nr = 15\n",
    "        fac = 1\n",
    "        prompt = f'Give me {nr*2} independent keywords to the topic {topic}.\\n\\n1.'\n",
    "        all_sentences.extend(gen_sentences(nr*2, fac, prompt))\n",
    "        with open(\"save.p\", \"wb\") as f:\n",
    "            pickle.dump(all_sentences, f)\n",
    "        print(f'Writing sentences not about {topic}.')\n",
    "        prompt = f'Give me {nr} topics fully unrelated to {topic}.\\n\\n1.'\n",
    "        all_sentences.extend(gen_sentences(nr, 2*fac, prompt))\n",
    "        with open(\"save.p\", \"wb\") as f:\n",
    "            pickle.dump(all_sentences, f)\n",
    "        print(all_sentences)\n",
    "        print(len(all_sentences))\n",
    "        print('Labelling sentences.')\n",
    "        labels = []\n",
    "        for i in range(len(all_sentences)):\n",
    "            if i < len(all_sentences)/2:\n",
    "                labels.append(True)\n",
    "            else:\n",
    "                labels.append(False)\n",
    "        data = [all_sentences, labels]\n",
    "        data = np.array(data).transpose()\n",
    "        mapping = []\n",
    "        uni = np.unique(data)\n",
    "        for i in uni:\n",
    "            mapping.append(np.where(data == i)[0][0])\n",
    "        data = data[mapping[1:]]\n",
    "        with open(\"save.p\", \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print('full data has been saved to \"save.p\".')\n",
    "    else:\n",
    "        # data = data[~pd.isnull(data[:,0])]  # doesn't work\n",
    "        mapping = []\n",
    "        uni = np.unique(data)\n",
    "        for i in uni:\n",
    "            mapping.append(np.where(data == i)[0][0])\n",
    "        data = data[mapping[1:]]\n",
    "    pd.DataFrame(data).to_csv(f\"{topic.replace(' ', '_')}_generated_data.csv\", index = False, header = ['sentences', 'labels'])\n",
    "    return pd.read_csv(f\"{topic.replace(' ', '_')}_generated_data.csv\")\n",
    "    # TODO needs better filters and better quality. Especially empty inputs need to be filtered out!\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    # test if this works with truncation=False\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def prepare_data_slowly(data, topic):\n",
    "    # data = data[data.review.str.split().str.len().le(64)]\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    # use sentence embedding to encode the reviews\n",
    "    # model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embedded_data = np.array([[0,0]])\n",
    "    # embedded_data = torch.load('embedded_data.pt')\n",
    "    k = 100\n",
    "    for i in range(math.ceil(len(np_data[0]) / k)):\n",
    "        reviews = long_roberta(list(np_data[0][k*i:k*i+k]))\n",
    "        labels = np_data[1][k*i:k*i+k]\n",
    "        # reviews = torch.tensor_split(reviews, 0, dim=0)\n",
    "        a = np.array([torch.tensor_split(reviews, len(reviews)), labels])\n",
    "        a = a.transpose()\n",
    "        embedded_data = np.append(embedded_data, a, axis=0)\n",
    "        if i == 0:\n",
    "            embedded_data = embedded_data[1:]\n",
    "        # embedded_data.extend(a)\n",
    "        torch.save(embedded_data, f'embedded_data_{topic}.pt')\n",
    "        print(f'saved {i+1} / {len(np_data[0]) / k}')\n",
    "    return embedded_data.transpose()\n",
    "\n",
    "\n",
    "def check_length(data):\n",
    "    def tokenize(sentences):\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=False, return_tensors='pt')\n",
    "        return encoded_input\n",
    "\n",
    "    sorted_data = data.reindex(data.sentences.str.len().sort_values().index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    for idx, row in sorted_data.iterrows():\n",
    "        length = len(tokenize(row.sentences)['input_ids'][0])\n",
    "        if length > 512:\n",
    "            print('Warning: Paragraph longer than 512 tokens therefore to long.')\n",
    "        elif length > 128:\n",
    "            print('Warning: Paragraph longer than 128 tokens therefore longer than recommended.')\n",
    "        elif length < 80:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "def analyse_full_data(data):\n",
    "    '''\n",
    "    max = data.review.str.len().sum()\n",
    "    print('Full set average length')\n",
    "    print(max / 50000)\n",
    "    data = data[data.review.str.split().str.len().le(64)]\n",
    "    max = data.review.str.len().sum()\n",
    "    print('Short set average length')\n",
    "    print(max / 50000)\n",
    "    '''\n",
    "\n",
    "    print('INFO')\n",
    "    data.info()\n",
    "    data.groupby(['labels']).describe()\n",
    "    print(f'Number of unique sentences: {data[\"sentences\"].nunique()}')\n",
    "    duplicates = data[data.duplicated()]\n",
    "    print(f'Number of duplicate rows:\\n{len(duplicates)}')\n",
    "    print(f'Check for nulls:\\n{data.isnull().sum()}')\n",
    "    sns.countplot(x=data['labels'])  # ploting distribution for easier understanding\n",
    "    print(data.head(3))\n",
    "\n",
    "    print('A few random examples from the dataset:')\n",
    "    # let's see how data is looklike\n",
    "    random_index = random.randint(0, data.shape[0] - 3)\n",
    "    for row in data[['sentences', 'labels']][random_index:random_index + 3].itertuples():\n",
    "        _, text, label = row\n",
    "        class_name = \"About topic\"\n",
    "        if label == 0:\n",
    "            class_name = \"Not about topic\"\n",
    "        print(f'TEXT: {text}')\n",
    "        print(f'LABEL: {label}')\n",
    "    # data contain so much garbage needs to be cleaned\n",
    "\n",
    "    truedata = data[data['labels'] == 1]\n",
    "    truedata = truedata['sentences']\n",
    "    falsedata = data[data['labels'] == 0]\n",
    "    falsedata = falsedata['sentences']\n",
    "\n",
    "    def wordcloud_draw(data, color, s):\n",
    "        words = ' '.join(data)\n",
    "        cleaned_word = \" \".join([word for word in words.split() if (word != 'movie' and word != 'film')])\n",
    "        wordcloud = WordCloud(stopwords=stopwords.words('english'), background_color=color, width=2500,\n",
    "                              height=2000).generate(cleaned_word)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(s)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.figure(figsize=[20, 10])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    wordcloud_draw(truedata, 'white', 'Most-common words about the topic')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    wordcloud_draw(falsedata, 'white', 'Most-common words not about the topic')\n",
    "    plt.show() # end wordcloud\n",
    "\n",
    "    data['text_word_count'] = data['sentences'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    numerical_feature_cols = ['text_word_count']  # numerical_feature_cols = data['text_word_count']\n",
    "\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    for i, col in enumerate(numerical_feature_cols):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=data, x=col, bins=50, color='#6495ED')\n",
    "        plt.title(f\"Distribution of Various word counts\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    for i, col in enumerate(numerical_feature_cols):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=data, x=col, hue='labels', bins=50)\n",
    "        plt.title(f\"Distribution of Various word counts with respect to target\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class TopicIdentifier:\n",
    "    def __init__(self):\n",
    "        self.running_loss = None\n",
    "        self.optimizer = None\n",
    "        self.dataloader = None\n",
    "        self.model = None\n",
    "        self.loss = None\n",
    "        self.dataframe = None\n",
    "        self.val_set = None\n",
    "        self.train_set = None\n",
    "        self.labels = None\n",
    "        self.sentences = None\n",
    "        self.embedded_data = None\n",
    "        self.raw_data = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def generate_training_data(self, topic, real=True):\n",
    "        if real:\n",
    "            self.raw_data = generate_data(topic, nr=15)\n",
    "        else:\n",
    "            data = load_data('save.p')\n",
    "            self.raw_data = generate_data(topic, data=data, nr=15)\n",
    "\n",
    "    def embedd_data(self, topic, real=True):\n",
    "        def get_element(arr):\n",
    "            return arr[0]\n",
    "        if real:\n",
    "            self.embedded_data = prepare_data_slowly(self.raw_data, topic)\n",
    "        else:\n",
    "            self.embedded_data = torch.load(f'embedded_data_{topic}.pt').transpose()\n",
    "        tpl = tuple(map(get_element, tuple(np.array_split(self.embedded_data[0], len(self.embedded_data[0])))))\n",
    "        self.sentences = torch.cat(tpl)\n",
    "        self.labels = self.embedded_data[1]\n",
    "        self.labels[self.labels == True] = 1.\n",
    "        self.labels[self.labels == False] = 0.\n",
    "        self.labels = np.expand_dims(self.labels, axis=1).astype('float32')\n",
    "        self.labels = torch.from_numpy(self.labels)\n",
    "        self.dataset = CustomTopicDataset(self.sentences, self.labels)\n",
    "\n",
    "    def analyse_training_data(self):\n",
    "        #check_length(self.raw_data)\n",
    "        analyse_full_data(self.raw_data)\n",
    "\n",
    "    def train(self, epochs=10, lr=0.001, val_frac=0.1, batch_size=25, loss=nn.BCELoss()):\n",
    "        def get_acc(pred, target):\n",
    "            pred_tag = torch.round(pred)\n",
    "\n",
    "            correct_results_sum = (pred_tag == target).sum().float()\n",
    "            acc = correct_results_sum / target.shape[0]\n",
    "            acc = torch.round(acc * 100)\n",
    "\n",
    "            return acc\n",
    "\n",
    "        val_len = int(round(len(self.dataset)*val_frac))\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(self.dataset, [len(self.dataset)-val_len, val_len])\n",
    "        self.dataloader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=True)\n",
    "        self.model = NeuralNetwork(self.dataset.shape)\n",
    "\n",
    "        self.loss = loss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        r2loss = R2Score()\n",
    "        mseloss = nn.MSELoss()\n",
    "        bceloss = nn.BCELoss()\n",
    "        accuracy = get_acc\n",
    "\n",
    "        history = History(self.val_set, self.train_set, self.model, r2loss=r2loss, mseloss=mseloss, accuracy=accuracy, bceloss=bceloss)\n",
    "\n",
    "        # main training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.running_loss = 0.\n",
    "            print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "            for step, (inputs, labels) in enumerate(self.dataloader):\n",
    "                y_pred = self.model(inputs)\n",
    "                l = self.loss(y_pred, labels)\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.running_loss += l.item()\n",
    "                # TODO: Make this dynamic to about 5 saves per epoch.\n",
    "                if (step + 1) % math.floor(len(self.dataloader)) == 0:  # if (step+1) % 100 == 0:\n",
    "                    print(f'current loss:\\t\\t{self.running_loss / 100}')\n",
    "                    self.running_loss = 0\n",
    "                    history.save(epoch + step / len(self.dataloader))\n",
    "                    # save current state of the model to history\n",
    "        now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i').replace('-', '_')\n",
    "        os.mkdir(f\"model_{now}\")\n",
    "        torch.save(self.model, f\"model_{now}/model.pt\")\n",
    "        print(f'Model saved to \"model_{now}/model.pt\"')\n",
    "        history.plot(f\"model_{now}\")\n",
    "        return history, self.model\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def try_model(model):\n",
    "    a = input('Please enter your input sentence: ')\n",
    "    a = long_roberta(a)\n",
    "    pred = model(a)\n",
    "    print(pred.item())\n",
    "    print('Where 1 is about the topic and 0 is not.\\n')\n",
    "\n",
    "\n",
    "def prompt_engineering_acc(topic, dataframe):\n",
    "    acc = []\n",
    "    for idx, row in dataframe.reset_index().iterrows():\n",
    "        inp = row['sentences']\n",
    "        out = row['labels']\n",
    "        prompt = f'Answer with either \"yes\" or \"no\". Is the following sentence about {topic}?\\n\\n{inp}\\n\\nAnswer:'\n",
    "        response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=0, max_tokens=2)\n",
    "        response = response['choices'][0]['text']\n",
    "        if response.lower().startswith(' y'):\n",
    "            response = 1\n",
    "        elif response.lower().startswith(' n'):\n",
    "            response = 0\n",
    "        else:\n",
    "            print(f'Bad response from openai: \"{response}\".')\n",
    "        if isinstance(response, int):\n",
    "            if response == out:\n",
    "                acc.append(1)\n",
    "            else:\n",
    "                acc.append(0)\n",
    "    return 100*sum(acc)/len(acc)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = 'biology'\n",
    "    topic2 = 'chemistry'\n",
    "    ti = TopicIdentifier()\n",
    "    t2 = TopicIdentifier()\n",
    "    ti.generate_training_data(topic)\n",
    "    t2.generate_training_data(topic2)\n",
    "    acc = prompt_engineering_acc(topic, ti.raw_data)\n",
    "    ac2 = prompt_engineering_acc(topic2, t2.raw_data)\n",
    "    print(f'{topic} prompt engineering: {acc}')\n",
    "    print(f'{topic2} prompt engineering: {ac2}')\n",
    "    print(f'{topic} analyse ----------------------------')\n",
    "    ti.analyse_training_data()\n",
    "    print(f'{topic2} analyse ----------------------------')\n",
    "    t2.analyse_training_data()\n",
    "    ti.embedd_data(topic)\n",
    "    t2.embedd_data(topic2)\n",
    "    history, biology = ti.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())\n",
    "    history2, chemistry = ti.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())\n",
    "    while True:\n",
    "        print('BIOLOGY:')\n",
    "        try_model(biology)\n",
    "        print('CHEMISTRY:')\n",
    "        try_model(chemistry)\n",
    "\n",
    "\n",
    "\n",
    "# Far higher diversity in not topic related samples needed. Normal conversation, random sequences of letters, etc.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the movie review code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO comment to everything its purpose\n",
    "\n",
    "\n",
    "# These are the start command when running this as jupyter notebook on colabs:\n",
    "\n",
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import math\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "\n",
    "class CustomMovieDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments):\n",
    "        self.x = reviews\n",
    "        self.y = sentiments\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class History():\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_'+i: []})\n",
    "            self.history.update({'tra_'+i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "\n",
    "    def save(self, step):\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_'+i: []})\n",
    "            short_history.update({'tra_'+i: []})\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():\n",
    "                    val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                    tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                    short_history['val_'+j].append(val_l)\n",
    "                    short_history['tra_'+j].append(tra_l)\n",
    "                self.model.train()\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            print(f'{i}:')\n",
    "            plt.show()\n",
    "            figures.append(fig)\n",
    "            plt.clf()\n",
    "        return figures\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, test_size=1000, train_batch_size=10, validation_batch_size=512, num_workers=2,\n",
    "         loss=None, data_factor=1):\n",
    "    if loss is None:\n",
    "        loss = nn.MSELoss()  # TODO pass loss as function object\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data = data.sample(frac=data_factor)\n",
    "    data, sentiments = prepare_data(data)\n",
    "\n",
    "    dataset = CustomMovieDataset(data, sentiments)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len(data) - test_size, test_size])\n",
    "    print(len(val_set))\n",
    "    print(len(train_set))\n",
    "\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=validation_batch_size, shuffle=True)\n",
    "    dataloader = DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    print(data)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    bceloss = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    history = History(val_set, train_set, model, r2loss=r2loss, mseloss=mseloss, accuracy=get_acc, bceloss=bceloss)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_l = 0\n",
    "        print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            y_pred = model(inputs)\n",
    "            # l = loss(y_pred, labels)\n",
    "            l = mseloss(y_pred, labels)\n",
    "            running_l += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (step + 1) % 50 == 0:  # if (step+1) % 100 == 0:\n",
    "                history.save(epoch * len(dataloader) + step)\n",
    "                print(f'training loss: {running_l / 50}')\n",
    "                running_l = 0\n",
    "    history.plot()\n",
    "\n",
    "\n",
    "def get_acc(pred, target):\n",
    "    pred_tag = torch.round(pred)\n",
    "\n",
    "    correct_results_sum = (pred_tag == target).sum().float()\n",
    "    acc = correct_results_sum / target.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    file_data = torch.load('embedded_reviews.pt')\n",
    "    print(type(file_data))\n",
    "    x = []\n",
    "    print(f'length of file_data: {len(file_data)}')\n",
    "    for i in file_data:\n",
    "        x.append(torch.from_numpy(i))\n",
    "    print(f'length of x: {len(x)}')\n",
    "    reviews = torch.cat(x)\n",
    "    # reviews = model.encode(np_data[0])\n",
    "    sentiments = np_data[1][:7100]\n",
    "    print(f'length of reviews: {len(reviews)}')\n",
    "    print(f'length of sentiments: {len(sentiments)}')\n",
    "    sentiments[sentiments == 'positive'] = [1.]\n",
    "    sentiments[sentiments == 'negative'] = [0.]\n",
    "    sents = []\n",
    "    for i in sentiments:\n",
    "        sents.append([i])\n",
    "    sentiments = np.array(sents, dtype=np.float32)\n",
    "    sentiments = torch.from_numpy(sentiments)\n",
    "    reviews = torch.tensor(reviews, dtype=torch.float32)\n",
    "    sentiments = torch.tensor(sentiments, dtype=torch.float32)  # line needed? dtype?\n",
    "    return reviews, sentiments\n",
    "\n",
    "def prepare_data_slowly():\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    # use sentence embedding to encode the reviews\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    all_reviews = []\n",
    "    k = 100\n",
    "    for i in range(round(len(np_data[0]) / k)):\n",
    "        reviews = model.encode(np_data[0][k*i:k*i+k])\n",
    "        all_reviews.append(reviews)\n",
    "        torch.save(all_reviews, 'embedded_reviews.pt')\n",
    "        print(f'saved {i+1} / {len(np_data[0]) / k}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # prepare_data_slowly()\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'test_size':500, # 1000  # 10% of full dataset\n",
    "        'train_batch_size':25,\n",
    "        'validation_batch_size':512,\n",
    "        'num_workers':2,\n",
    "        'loss':nn.BCELoss(),\n",
    "        'data_factor': 1\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    # for jupyter:\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short version of the movie review code to test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO comment to everything its purpose\n",
    "\n",
    "\n",
    "# These are the start command when running this as jupyter notebook on colabs:\n",
    "\n",
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomMovieDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments):\n",
    "        self.x = reviews\n",
    "        self.y = sentiments\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, train_batch_size=10, loss=None):\n",
    "    if loss is None:\n",
    "        loss = nn.MSELoss()  # TODO pass loss as function object\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data, sentiments = prepare_data(data)\n",
    "\n",
    "    dataset = CustomMovieDataset(data, sentiments)\n",
    "\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    print(data)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    bceloss = nn.BCELoss()\n",
    "\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_l = 0\n",
    "        print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            y_pred = model(inputs)\n",
    "            # l = loss(y_pred, labels)\n",
    "            l = loss(y_pred, labels)\n",
    "            running_l += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (step + 1) % 50 == 0:  # if (step+1) % 100 == 0:\n",
    "                print(f'training loss: {running_l / 50}')\n",
    "                running_l = 0\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    file_data = torch.load('embedded_reviews.pt')\n",
    "    print(type(file_data))\n",
    "    x = []\n",
    "    print(f'length of file_data: {len(file_data)}')\n",
    "    for i in file_data:\n",
    "        x.append(torch.from_numpy(i))\n",
    "    print(f'length of x: {len(x)}')\n",
    "    reviews = torch.cat(x)\n",
    "    # reviews = model.encode(np_data[0])\n",
    "    sentiments = np_data[1][:7100]\n",
    "    print(f'length of reviews: {len(reviews)}')\n",
    "    print(f'length of sentiments: {len(sentiments)}')\n",
    "    sentiments[sentiments == 'positive'] = [1.]\n",
    "    sentiments[sentiments == 'negative'] = [0.]\n",
    "    sents = []\n",
    "    for i in sentiments:\n",
    "        sents.append([i])\n",
    "    sentiments = np.array(sents, dtype=np.float32)\n",
    "    sentiments = torch.from_numpy(sentiments)\n",
    "    reviews = torch.tensor(reviews, dtype=torch.float32)\n",
    "    sentiments = torch.tensor(sentiments, dtype=torch.float32)  # line needed? dtype?\n",
    "    return reviews, sentiments\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # prepare_data_slowly()\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'train_batch_size':25,\n",
    "        'loss':nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    # for jupyter:\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full diamond price estimator code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting.\n",
      "Requirement already satisfied: transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.5.4)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.5.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction beforehand: 306.19880845178545\t\tcorrect was: 2821.999999999999\n",
      "Starting new batch 1/1\n"
     ]
    }
   ],
   "source": [
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime as d\n",
    "import jupyter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi()\n",
    "# api.authenticate() # comment out for jupyter\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import R2Score\n",
    "# from torchmetrics.functional import r2_score\n",
    "\n",
    "class CustomDiamondDataset(Dataset):\n",
    "    def __init__(self, data, prices):\n",
    "        self.x = data\n",
    "        self.y = prices\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, test_size=1000, train_batch_size=10, validation_batch_size=512, num_workers=2, loss=None, optimizer=None, data_factor=1):\n",
    "    if loss is None:\n",
    "        loss = 'nn.MSELoss()'\n",
    "    if optimizer is None:\n",
    "        optimizer = 'torch.optim.SGD(model.parameters(), lr=learning_rate)'\n",
    "\n",
    "    writer = SummaryWriter('runs/diamond')\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/diamonds.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data = data.sample(frac=data_factor)\n",
    "    data, prices, maxi, mini = normalize_data(data)\n",
    "    # data = data[:100]\n",
    "    # prices = prices[:100]\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "    dataset = CustomDiamondDataset(data, prices)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len(data)-test_size, test_size])\n",
    "\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=validation_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    dataloader = DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    loss = eval(loss)\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    optimizer = eval(optimizer)\n",
    "    # loss = nn.MSELoss()  # try others: r squared metric scale from -1 (opposite) to 1 (ideal) to infinite (wrong again); accuracy error\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    val_mse_loss = []\n",
    "    val_r2loss = []\n",
    "    val_real_price_percentage_loss = []\n",
    "    percentages = []\n",
    "    training_mse_loss = []\n",
    "    training_r2loss = []\n",
    "    training_real_price_percentage_loss = []\n",
    "    x_axis = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.\n",
    "        running_r2loss = 0.\n",
    "        print(f'Starting new batch {epoch+1}/{epochs}')\n",
    "        # check_accuracy(valloader, model, maxi, mini)\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            # calculate r squarred loss\n",
    "            y_pred = model(inputs)\n",
    "            for pred, label in zip(y_pred, labels):\n",
    "                pr = get_real_price(pred, maxi, mini)\n",
    "                la = get_real_price(label, maxi, mini)\n",
    "                percentages.append(abs(pr-la)/la)\n",
    "            l = loss(y_pred, labels)\n",
    "            # msel = mseloss(y_pred, labels)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += l.item()  # l.item()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                x = r2loss(y_pred, labels).item()\n",
    "                if x >= 1:\n",
    "                    print('x >= 1')\n",
    "                running_r2loss += x\n",
    "                model.train()\n",
    "            if (step+1) % 100 == 0: # if (step+1) % 100 == 0:\n",
    "                mse_l, r2_l, percent_l = evaluate_model(model, valloader, maxi, mini, loss, r2loss)\n",
    "                val_mse_loss.append(mse_l)\n",
    "                val_r2loss.append(r2_l)\n",
    "                val_real_price_percentage_loss.append(percent_l)\n",
    "                training_mse_loss.append(running_loss / 100)\n",
    "                training_r2loss.append(running_r2loss / 100)\n",
    "                training_real_price_percentage_loss.append(sum(percentages)/len(percentages)*100) # to static. Why?\n",
    "                x_axis.append(epoch*len(dataloader) + step)\n",
    "                writer.add_scalar('training_mse_loss', running_loss / 100, epoch*len(dataloader) + step)\n",
    "                writer.add_scalar('training_real_price_percentage_loss', sum(percentages)/len(percentages)*100, epoch*len(dataloader) + step)\n",
    "                running_loss = 0.\n",
    "                running_r2loss = 0.\n",
    "                percentages = []\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoint.pth')\n",
    "        # to load:\n",
    "        # loaded_checkpoint =torch.load('checkpoint.pth')\n",
    "    writer.close()\n",
    "    args = [x_axis, val_mse_loss, training_mse_loss, val_r2loss, training_r2loss, val_real_price_percentage_loss, training_real_price_percentage_loss]\n",
    "    print(args)\n",
    "    graph(*args)\n",
    "    file = '2model.pth'\n",
    "    torch.save(model.state_dict(), file)\n",
    "\n",
    "def graph(x_axis, val_mse_loss, training_mse_loss, val_r2loss, training_r2loss, val_real_price_percentage_loss, training_real_price_percentage_loss):\n",
    "    path = os.path.abspath(os.getcwd())\n",
    "    ra = str(random.randint(1,100))\n",
    "    ver = str(2)\n",
    "    now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_mse_loss, 'b') # ? (0)\n",
    "    ax.plot(x_axis, training_mse_loss, 'r')  # 0\n",
    "    print('mse loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}mse_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_r2loss, 'b') # ? (0)\n",
    "    ax.plot(x_axis, training_r2loss, 'r') # ? (0)\n",
    "    print('r2 loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}r2_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_real_price_percentage_loss, 'b') # good\n",
    "    ax.plot(x_axis, training_real_price_percentage_loss, 'r') # good\n",
    "    print('real price percentage loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}perc_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def load_model(data, file = '1model.pth'):\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "    loaded_model = NeuralNetwork(len(data[0]))\n",
    "    loaded_model.load_state_dict(torch.load(file))\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def evaluate_model(model, valloader, maxi, mini, loss, r2loss):\n",
    "    # print('\\n\\nStart evaluating')\n",
    "    with torch.no_grad():\n",
    "        # try using accuracy in addition to loss\n",
    "        model.eval()\n",
    "        percentages = []\n",
    "        avg_mse_loss = []\n",
    "        avg_r2_loss = []\n",
    "        for step, (inputs, labels) in enumerate(valloader):\n",
    "            mistakes = []\n",
    "            y_pred = model(inputs)\n",
    "            for pred, label in zip(y_pred, labels):\n",
    "                pr = get_real_price(pred, maxi, mini)\n",
    "                la = get_real_price(label, maxi, mini)\n",
    "                # print(f'Estimation: {p}; True: {la}')\n",
    "                mistakes.append(abs(pr-la))\n",
    "                percentages.append(abs(pr-la)/la)\n",
    "            l = loss(y_pred, labels)\n",
    "            r2l = r2loss(y_pred, labels)\n",
    "            # print(f'Average real-price error for this batch was: \\t\\t\\t\\t\\t{sum(mistakes)/len(mistakes)}.')\n",
    "            # print(f'Average real-price error relative to the price in percent was: '\n",
    "            #       f'\\t{sum(percentages)/len(percentages)*100}%.')\n",
    "            # print(f'Average loss for this batch was \\t\\t\\t\\t\\t\\t\\t\\t{l.item()}\\n')\n",
    "            avg_mse_loss.append(l.item())\n",
    "            avg_r2_loss.append(r2l.item())\n",
    "        model.train()\n",
    "        return sum(avg_mse_loss)/len(avg_mse_loss), sum(avg_r2_loss)/len(avg_r2_loss), sum(percentages)/len(percentages)*100\n",
    "\n",
    "    # Graph test over training !\n",
    "    # plot everything on the graph, accuracy, MSEloss, R^2loss, percentage_price%\n",
    "\n",
    "\n",
    "\n",
    "# check accuracy causes Error - not used\n",
    "def check_accuracy(loader, model, maxi, mini):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        aver = []\n",
    "        for x, y in loader:\n",
    "            correct = get_real_price(y.item(), maxi, mini)\n",
    "            resp = model(x)\n",
    "            price = get_real_price(resp.item(), maxi, mini)\n",
    "            aver.append(abs(correct - price))\n",
    "        model.train()\n",
    "        print(sum(aver)/len(aver))\n",
    "        return sum(aver)/len(aver)\n",
    "            #scores = model(x)\n",
    "            #res = scores.unsqueeze(1) - y\n",
    "            #a = torch.mean(res).item()\n",
    "            #aver.append(a)\n",
    "\n",
    "            #_, predictions = scores.max(1)\n",
    "            #num_correct += (predictions == y).sum()\n",
    "            #num_samples += predictions.size(0)\n",
    "\n",
    "        # print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "\n",
    "\n",
    "# both robertas fully copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "def short_roberta(sentences):\n",
    "    # sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = f.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def get_real_price(val, maxi, mini):\n",
    "    x = (maxi-mini) * val + mini\n",
    "    return math.exp(x)\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    # max / min normalization the dataset\n",
    "    # scalars stay normal columns, categories get different columns - one hot encoding\n",
    "    # price on a logarythmic scale\n",
    "\n",
    "    '''\n",
    "    let's build a tensor with the following dimensions:\n",
    "        carat\n",
    "        *cut* (hot encoding)\n",
    "            ideal\n",
    "            premium\n",
    "            good\n",
    "            very good\n",
    "            fair\n",
    "        colour (auch hot encoding)\n",
    "        clarity (dito)\n",
    "        depth\n",
    "        table\n",
    "        price\n",
    "        x\n",
    "        y\n",
    "        z\n",
    "    then min max the full thing.\n",
    "    '''\n",
    "    def onehot():\n",
    "        nb_classes = 6\n",
    "        arr = np.array([[2, 3, 4, 0]])\n",
    "        targets = arr.reshape(-1)\n",
    "        one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        return one_hot_targets\n",
    "\n",
    "    onehot()\n",
    "    np_data = data.to_numpy()\n",
    "\n",
    "    cut_index = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\n",
    "    colour_index = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D':6}\n",
    "    # clarity: (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best))\n",
    "    clarity_index = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1':6, 'IF': 7}\n",
    "    indeces = [cut_index, colour_index, clarity_index]\n",
    "\n",
    "\n",
    "    # carat, cut (5), colour (7), clarity (8), depth, table, price, x, y\n",
    "    new_array = []\n",
    "    prices = []\n",
    "    for i, diamond in enumerate(np_data):\n",
    "        diamond = diamond[1:]\n",
    "        new_diamond = [diamond[0]]\n",
    "        for j in range(3):\n",
    "            index = indeces[j][diamond[j+1]]\n",
    "            zeros = [0.]*len(indeces[j].keys())\n",
    "            zeros[index] = 1.\n",
    "            for k in zeros:\n",
    "                new_diamond.append(k)\n",
    "        for j in [4, 5, 7, 8]:\n",
    "            new_diamond.append(diamond[j])\n",
    "        new_array.append(new_diamond)\n",
    "        prices.append(math.log(diamond[6]))\n",
    "\n",
    "    maxi = max(prices)\n",
    "    mini = min(prices)\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(new_array)\n",
    "    prices = pd.DataFrame(prices)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    return normalized_data, normalized_prices, maxi, mini\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'test_size':1000, # 1000\n",
    "        'train_batch_size':10,\n",
    "        'validation_batch_size':512,\n",
    "        'num_workers':2,\n",
    "        'loss':'nn.MSELoss()',\n",
    "        'optimizer':'torch.optim.SGD(model.parameters(), lr=learning_rate)',\n",
    "        'data_factor': 1\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    #args = [[29, 59, 89], [0.04725663047283888, 0.04288289994001389, 0.03785799648612738], [0.03140254817903042, 0.01449822638183832, 0.013357452619820832], [0.21161172389984131, 0.3071813404560089, 0.3442371547222137], [-0.1830847430229187, 0.05015255331993103, 0.08432324945926667], [86.34664962859036, 78.04877220648744, 74.22272207896643], [67.15759899285841, 91.4195255019661, 84.78499436740307]]\n",
    "    #graph(*args)\n",
    "\n",
    "    # for jupyter:\n",
    "    #   comment out saving of graph (and not model?).\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
