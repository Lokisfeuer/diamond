{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!export KAGGLE_USERNAME=\"juliavonoertzen\"\n",
    "!export KAGGLE_KEY=\"cd10da87d806a69edbcb2f6f0cbf708e\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import jupyter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDiamondDataset(Dataset):\n",
    "    def __init__(self, data, prices):\n",
    "        self.x = data\n",
    "        self.y = prices\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = pd.read_csv('diamonds.csv')\n",
    "    data, prices, maxi, mini = normalize_data(data)\n",
    "    # data = data[:100]\n",
    "    # prices = prices[:100]\n",
    "    test_input = torch.tensor(np.array([data[2]]), dtype=torch.float32)\n",
    "    price = get_real_price(prices[2][0], maxi, mini)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "    dataset = CustomDiamondDataset(data, prices)\n",
    "    print(len(data[0]))\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    print(type(data))\n",
    "    print(type(prices))\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    epochs = 100\n",
    "\n",
    "    print(f'Prediction beforehand: {get_real_price(model(test_input).item(), maxi, mini)}\\n\\ncorrect was: {price}')\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Starting new batch {epoch+1}/{epochs}')\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            y_pred = model(inputs)\n",
    "            l = loss(labels, y_pred)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(f'Prediction afterwards: {get_real_price(model(test_input).item(), maxi, mini)}\\n\\ncorrect was: {price}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    # don't forget to normalize data - hot encoding\n",
    "        # max / min normalization the dataset\n",
    "    # scalars stay normal columns, categories get different columns\n",
    "    # put price on a logarythmic scale\n",
    "\n",
    "    '''\n",
    "    lets build a tensor with the following dimensions:\n",
    "        carat\n",
    "        *cut* (hot encoding)\n",
    "            ideal\n",
    "            premium\n",
    "            good\n",
    "            very good\n",
    "            fair\n",
    "        colour (auch hot encoding)\n",
    "        clarity (dito)\n",
    "        depth\n",
    "        table\n",
    "        price\n",
    "        x\n",
    "        y\n",
    "        z\n",
    "    then min max the full thing.\n",
    "    '''\n",
    "    def onehot():\n",
    "        nb_classes = 6\n",
    "        arr = np.array([[2, 3, 4, 0]])\n",
    "        targets = arr.reshape(-1)\n",
    "        one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        return one_hot_targets\n",
    "\n",
    "    onehot()\n",
    "    np_data = data.to_numpy()\n",
    "\n",
    "    cut_index = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\n",
    "    colour_index = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D':6}\n",
    "    # clarity: (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best))\n",
    "    clarity_index = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1':6, 'IF': 7}\n",
    "    indeces = [cut_index, colour_index, clarity_index]\n",
    "\n",
    "\n",
    "    # carat, cut (5), colour (7), clarity (8), depth, table, price, x, y\n",
    "    new_array = []\n",
    "    prices = []\n",
    "    for i, diamond in enumerate(np_data):\n",
    "        diamond = diamond[1:]\n",
    "        new_diamond = [diamond[0]]\n",
    "        for j in range(3):\n",
    "            index = indeces[j][diamond[j+1]]\n",
    "            zeros = [0.]*len(indeces[j].keys())\n",
    "            zeros[index] = 1.\n",
    "            for k in zeros:\n",
    "                new_diamond.append(k)\n",
    "        for j in [4, 5, 7, 8]:\n",
    "            new_diamond.append(diamond[j])\n",
    "        new_array.append(new_diamond)\n",
    "        prices.append(math.log(diamond[6]))\n",
    "\n",
    "    maxi = max(prices)\n",
    "    mini = min(prices)\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(new_array)\n",
    "    prices = pd.DataFrame(prices)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    return normalized_data, normalized_prices, maxi, mini\n",
    "\n",
    "def get_real_price(val, maxi, mini):\n",
    "    x = (maxi-mini) * val + mini\n",
    "    return math.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-find-test-accuracy-after-training/88962\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "\n",
    "    model.train()\n",
    "    return float(num_correct)/float(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/diamonds.csv'\n",
    "data = pd.read_csv(url)\n",
    "data, prices, maxi, mini = normalize_data(data)\n",
    "\n",
    "test_input = torch.tensor(np.array([data[2]]), dtype=torch.float32)\n",
    "price = get_real_price(prices[2][0], maxi, mini)\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "prices = torch.tensor(prices, dtype=torch.float32)\n",
    "dataset = CustomDiamondDataset(data, prices)\n",
    "test_size = 5000\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [len(data)-test_size, test_size])\n",
    "\n",
    "valloader = DataLoader(dataset=val_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "dataloader = DataLoader(dataset=train_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "model = NeuralNetwork(len(data[0]))\n",
    "learning_rate = 0.01\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "model = NeuralNetwork(len(data[0]))\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Prediction beforehand: {get_real_price(model(test_input).item(), maxi, mini)}\\n\\ncorrect was: {price}')\n",
    "for epoch in range(epochs):\n",
    "    print(f'Starting new batch {epoch+1}/{epochs}')\n",
    "    for step, (inputs, labels) in enumerate(dataloader):\n",
    "        y_pred = model(inputs)\n",
    "        l = loss(labels, y_pred)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "print(f'Prediction afterwards: {get_real_price(model(test_input).item(), maxi, mini)}\\n\\ncorrect was: {price}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now its random code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting.\n",
      "Requirement already satisfied: transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.5.4)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.5.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction beforehand: 306.19880845178545\t\tcorrect was: 2821.999999999999\n",
      "Starting new batch 1/1\n"
     ]
    }
   ],
   "source": [
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "# !export KAGGLE_USERNAME=\"juliavonoertzen\"\n",
    "# !export KAGGLE_KEY=\"cd10da87d806a69edbcb2f6f0cbf708e\"\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import jupyter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class CustomDiamondDataset(Dataset):\n",
    "    def __init__(self, data, prices):\n",
    "        self.x = data\n",
    "        self.y = prices\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/diamonds.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data, prices, maxi, mini = normalize_data(data)\n",
    "    # data = data[:100]\n",
    "    # prices = prices[:100]\n",
    "    test_input = torch.tensor(np.array([data[500]]), dtype=torch.float32)\n",
    "    price = get_real_price(prices[500][0], maxi, mini)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "    dataset = CustomDiamondDataset(data, prices)\n",
    "    test_size = 5000\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len(data)-test_size, test_size])\n",
    "\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "    dataloader = DataLoader(dataset=train_set, batch_size=10, shuffle=True, num_workers=2)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    learning_rate = 0.01\n",
    "    loss = nn.MSELoss()  # try others: r squared metric scale from -1 (opposite) to 1 (ideal) to infinite (wrong again); accuracy error\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    epochs = 1\n",
    "\n",
    "    print(f'Prediction beforehand: {get_real_price(model(test_input).item(), maxi, mini)}\\t\\tcorrect was: {price}')\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Starting new batch {epoch+1}/{epochs}')\n",
    "        avg_loss = 0.\n",
    "        # check_accuracy(valloader, model, maxi, mini)\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            # calculate r squarred loss\n",
    "            y_pred = model(inputs)\n",
    "            l = loss(y_pred, labels)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss += l.item()  # l.item()\n",
    "        print(avg_loss/len(dataloader))\n",
    "    print(f'Prediction afterwards: {get_real_price(model(test_input).item(), maxi, mini)}\\t\\tcorrect was: {price}')\n",
    "    print('\\n\\nStart evaluating')\n",
    "    with torch.no_grad():\n",
    "        # try using accuracy in addition to loss\n",
    "        model.eval()\n",
    "        avg_loss = 0.\n",
    "        for step, (inputs, labels) in enumerate(valloader):\n",
    "            mistakes = []\n",
    "            percentages = []\n",
    "            y_pred = model(inputs)\n",
    "            for pred, label in zip(y_pred, labels):\n",
    "                pr = get_real_price(pred, maxi, mini)\n",
    "                la = get_real_price(label, maxi, mini)\n",
    "                # print(f'Estimation: {p}; True: {la}')\n",
    "                mistakes.append(abs(pr-la))\n",
    "                percentages.append(abs(pr-la)/la)\n",
    "            l = loss(y_pred, labels)\n",
    "            print(f'Average real-price error for this batch was: \\t\\t\\t\\t\\t{sum(mistakes)/len(mistakes)}.')\n",
    "            print(f'Average real-price error relative to the price in percent was: '\n",
    "                  f'\\t{sum(percentages)/len(percentages)*100}%.')\n",
    "            print(f'Average loss for this batch was \\t\\t\\t\\t\\t\\t\\t\\t{l.item()}\\n')\n",
    "            avg_loss += l.item()  # l.item()\n",
    "        print(f'Overall average loss was: {avg_loss / len(valloader)}')\n",
    "        model.train()\n",
    "    # Graph test over training !\n",
    "    # plot everything on the graph, accuracy, MSEloss, R^2loss, percentage_price%\n",
    "    '''\n",
    "    n_samples, n_features = data.shape\n",
    "    input_size = n_features\n",
    "    model = nn.Linear(input_size, 1) # correct this !\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "    loss = nn.MSELoss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for i in range(100):\n",
    "        y_pred = model(X)\n",
    "        l = loss(Y, y_pred)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # output = model(input).item()\n",
    "    # print(type(data))\n",
    "    # print(prices)\n",
    "    # dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "    '''\n",
    "\n",
    "\n",
    "# check accuracy causes Error - not used\n",
    "def check_accuracy(loader, model, maxi, mini):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        aver = []\n",
    "        for x, y in loader:\n",
    "            correct = get_real_price(y.item(), maxi, mini)\n",
    "            resp = model(x)\n",
    "            price = get_real_price(resp.item(), maxi, mini)\n",
    "            aver.append(abs(correct - price))\n",
    "        model.train()\n",
    "        print(sum(aver)/len(aver))\n",
    "        return sum(aver)/len(aver)\n",
    "            #scores = model(x)\n",
    "            #res = scores.unsqueeze(1) - y\n",
    "            #a = torch.mean(res).item()\n",
    "            #aver.append(a)\n",
    "\n",
    "            #_, predictions = scores.max(1)\n",
    "            #num_correct += (predictions == y).sum()\n",
    "            #num_samples += predictions.size(0)\n",
    "\n",
    "        # print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "\n",
    "\n",
    "# both robertas fully copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "def short_roberta(sentences):\n",
    "    # sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = f.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def get_real_price(val, maxi, mini):\n",
    "    x = (maxi-mini) * val + mini\n",
    "    return math.exp(x)\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    # max / min normalization the dataset\n",
    "    # scalars stay normal columns, categories get different columns - one hot encoding\n",
    "    # price on a logarythmic scale\n",
    "\n",
    "    '''\n",
    "    let's build a tensor with the following dimensions:\n",
    "        carat\n",
    "        *cut* (hot encoding)\n",
    "            ideal\n",
    "            premium\n",
    "            good\n",
    "            very good\n",
    "            fair\n",
    "        colour (auch hot encoding)\n",
    "        clarity (dito)\n",
    "        depth\n",
    "        table\n",
    "        price\n",
    "        x\n",
    "        y\n",
    "        z\n",
    "    then min max the full thing.\n",
    "    '''\n",
    "    def onehot():\n",
    "        nb_classes = 6\n",
    "        arr = np.array([[2, 3, 4, 0]])\n",
    "        targets = arr.reshape(-1)\n",
    "        one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        return one_hot_targets\n",
    "\n",
    "    onehot()\n",
    "    np_data = data.to_numpy()\n",
    "\n",
    "    cut_index = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\n",
    "    colour_index = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D':6}\n",
    "    # clarity: (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best))\n",
    "    clarity_index = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1':6, 'IF': 7}\n",
    "    indeces = [cut_index, colour_index, clarity_index]\n",
    "\n",
    "\n",
    "    # carat, cut (5), colour (7), clarity (8), depth, table, price, x, y\n",
    "    new_array = []\n",
    "    prices = []\n",
    "    for i, diamond in enumerate(np_data):\n",
    "        diamond = diamond[1:]\n",
    "        new_diamond = [diamond[0]]\n",
    "        for j in range(3):\n",
    "            index = indeces[j][diamond[j+1]]\n",
    "            zeros = [0.]*len(indeces[j].keys())\n",
    "            zeros[index] = 1.\n",
    "            for k in zeros:\n",
    "                new_diamond.append(k)\n",
    "        for j in [4, 5, 7, 8]:\n",
    "            new_diamond.append(diamond[j])\n",
    "        new_array.append(new_diamond)\n",
    "        prices.append(math.log(diamond[6]))\n",
    "\n",
    "    maxi = max(prices)\n",
    "    mini = min(prices)\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(new_array)\n",
    "    prices = pd.DataFrame(prices)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    return normalized_data, normalized_prices, maxi, mini\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
