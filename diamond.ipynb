{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is the movie review code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO comment to everything its purpose\n",
    "\n",
    "\n",
    "# These are the start command when running this as jupyter notebook on colabs:\n",
    "\n",
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import math\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "\n",
    "class CustomMovieDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments):\n",
    "        self.x = reviews\n",
    "        self.y = sentiments\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class History():\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_'+i: []})\n",
    "            self.history.update({'tra_'+i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "\n",
    "    def save(self, step):\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_'+i: []})\n",
    "            short_history.update({'tra_'+i: []})\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():\n",
    "                    val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                    tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                    short_history['val_'+j].append(val_l)\n",
    "                    short_history['tra_'+j].append(tra_l)\n",
    "                self.model.train()\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            print(f'{i}:')\n",
    "            plt.show()\n",
    "            figures.append(fig)\n",
    "            plt.clf()\n",
    "        return figures\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, test_size=1000, train_batch_size=10, validation_batch_size=512, num_workers=2,\n",
    "         loss=None, data_factor=1):\n",
    "    if loss is None:\n",
    "        loss = nn.MSELoss()  # TODO pass loss as function object\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data = data.sample(frac=data_factor)\n",
    "    data, sentiments = prepare_data(data)\n",
    "\n",
    "    dataset = CustomMovieDataset(data, sentiments)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len(data) - test_size, test_size])\n",
    "    print(len(val_set))\n",
    "    print(len(train_set))\n",
    "\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=validation_batch_size, shuffle=True)\n",
    "    dataloader = DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)\n",
    "    print(data)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    bceloss = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    history = History(val_set, train_set, model, r2loss=r2loss, mseloss=mseloss, accuracy=get_acc, bceloss=bceloss)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_l = 0\n",
    "        print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            y_pred = model(inputs)\n",
    "            # l = loss(y_pred, labels)\n",
    "            l = mseloss(y_pred, labels)\n",
    "            running_l += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (step + 1) % 50 == 0:  # if (step+1) % 100 == 0:\n",
    "                history.save(epoch * len(dataloader) + step)\n",
    "                print(f'training loss: {running_l / 50}')\n",
    "                running_l = 0\n",
    "    history.plot()\n",
    "\n",
    "\n",
    "def get_acc(pred, target):\n",
    "    pred_tag = torch.round(pred)\n",
    "\n",
    "    correct_results_sum = (pred_tag == target).sum().float()\n",
    "    acc = correct_results_sum / target.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    file_data = torch.load('embedded_reviews.pt')\n",
    "    print(type(file_data))\n",
    "    x = []\n",
    "    print(f'length of file_data: {len(file_data)}')\n",
    "    for i in file_data:\n",
    "        x.append(torch.from_numpy(i))\n",
    "    print(f'length of x: {len(x)}')\n",
    "    reviews = torch.cat(x)\n",
    "    # reviews = model.encode(np_data[0])\n",
    "    sentiments = np_data[1][:7100]\n",
    "    print(f'length of reviews: {len(reviews)}')\n",
    "    print(f'length of sentiments: {len(sentiments)}')\n",
    "    sentiments[sentiments == 'positive'] = [1.]\n",
    "    sentiments[sentiments == 'negative'] = [0.]\n",
    "    sents = []\n",
    "    for i in sentiments:\n",
    "        sents.append([i])\n",
    "    sentiments = np.array(sents, dtype=np.float32)\n",
    "    sentiments = torch.from_numpy(sentiments)\n",
    "    reviews = torch.tensor(reviews, dtype=torch.float32)\n",
    "    sentiments = torch.tensor(sentiments, dtype=torch.float32)  # line needed? dtype?\n",
    "    return reviews, sentiments\n",
    "\n",
    "def prepare_data_slowly():\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    # use sentence embedding to encode the reviews\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    all_reviews = []\n",
    "    k = 100\n",
    "    for i in range(round(len(np_data[0]) / k)):\n",
    "        reviews = model.encode(np_data[0][k*i:k*i+k])\n",
    "        all_reviews.append(reviews)\n",
    "        torch.save(all_reviews, 'embedded_reviews.pt')\n",
    "        print(f'saved {i+1} / {len(np_data[0]) / k}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # prepare_data_slowly()\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'test_size':500, # 1000  # 10% of full dataset\n",
    "        'train_batch_size':25,\n",
    "        'validation_batch_size':512,\n",
    "        'num_workers':2,\n",
    "        'loss':nn.BCELoss(),\n",
    "        'data_factor': 1\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    # for jupyter:\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short version of the movie review code to test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO comment to everything its purpose\n",
    "\n",
    "\n",
    "# These are the start command when running this as jupyter notebook on colabs:\n",
    "\n",
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomMovieDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments):\n",
    "        self.x = reviews\n",
    "        self.y = sentiments\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, train_batch_size=10, loss=None):\n",
    "    if loss is None:\n",
    "        loss = nn.MSELoss()  # TODO pass loss as function object\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/imdbdataset.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data, sentiments = prepare_data(data)\n",
    "\n",
    "    dataset = CustomMovieDataset(data, sentiments)\n",
    "\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    print(data)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    bceloss = nn.BCELoss()\n",
    "\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_l = 0\n",
    "        print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            y_pred = model(inputs)\n",
    "            # l = loss(y_pred, labels)\n",
    "            l = loss(y_pred, labels)\n",
    "            running_l += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (step + 1) % 50 == 0:  # if (step+1) % 100 == 0:\n",
    "                print(f'training loss: {running_l / 50}')\n",
    "                running_l = 0\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    file_data = torch.load('embedded_reviews.pt')\n",
    "    print(type(file_data))\n",
    "    x = []\n",
    "    print(f'length of file_data: {len(file_data)}')\n",
    "    for i in file_data:\n",
    "        x.append(torch.from_numpy(i))\n",
    "    print(f'length of x: {len(x)}')\n",
    "    reviews = torch.cat(x)\n",
    "    # reviews = model.encode(np_data[0])\n",
    "    sentiments = np_data[1][:7100]\n",
    "    print(f'length of reviews: {len(reviews)}')\n",
    "    print(f'length of sentiments: {len(sentiments)}')\n",
    "    sentiments[sentiments == 'positive'] = [1.]\n",
    "    sentiments[sentiments == 'negative'] = [0.]\n",
    "    sents = []\n",
    "    for i in sentiments:\n",
    "        sents.append([i])\n",
    "    sentiments = np.array(sents, dtype=np.float32)\n",
    "    sentiments = torch.from_numpy(sentiments)\n",
    "    reviews = torch.tensor(reviews, dtype=torch.float32)\n",
    "    sentiments = torch.tensor(sentiments, dtype=torch.float32)  # line needed? dtype?\n",
    "    return reviews, sentiments\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # prepare_data_slowly()\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'test_size':500, # 1000  # 10% of full dataset\n",
    "        'train_batch_size':25,\n",
    "        'validation_batch_size':512,\n",
    "        'num_workers':2,\n",
    "        'loss':nn.BCEWithLogitsLoss(),\n",
    "        'data_factor': 1\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    # for jupyter:\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full diamond price estimator code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting.\n",
      "Requirement already satisfied: transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.5.4)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.5.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\thede\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction beforehand: 306.19880845178545\t\tcorrect was: 2821.999999999999\n",
      "Starting new batch 1/1\n"
     ]
    }
   ],
   "source": [
    "print('starting.')\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime as d\n",
    "import jupyter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi()\n",
    "# api.authenticate() # comment out for jupyter\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import R2Score\n",
    "# from torchmetrics.functional import r2_score\n",
    "\n",
    "class CustomDiamondDataset(Dataset):\n",
    "    def __init__(self, data, prices):\n",
    "        self.x = data\n",
    "        self.y = prices\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def main(epochs=10, learning_rate=0.01, test_size=1000, train_batch_size=10, validation_batch_size=512, num_workers=2, loss=None, optimizer=None, data_factor=1):\n",
    "    if loss is None:\n",
    "        loss = 'nn.MSELoss()'\n",
    "    if optimizer is None:\n",
    "        optimizer = 'torch.optim.SGD(model.parameters(), lr=learning_rate)'\n",
    "\n",
    "    writer = SummaryWriter('runs/diamond')\n",
    "    url = 'https://raw.githubusercontent.com/Lokisfeuer/diamond/master/diamonds.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data = data.sample(frac=data_factor)\n",
    "    data, prices, maxi, mini = normalize_data(data)\n",
    "    # data = data[:100]\n",
    "    # prices = prices[:100]\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    prices = torch.tensor(prices, dtype=torch.float32)\n",
    "    dataset = CustomDiamondDataset(data, prices)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len(data)-test_size, test_size])\n",
    "\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=validation_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    dataloader = DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    model = NeuralNetwork(len(data[0]))\n",
    "    loss = eval(loss)\n",
    "    r2loss = R2Score()\n",
    "    mseloss = nn.MSELoss()\n",
    "    optimizer = eval(optimizer)\n",
    "    # loss = nn.MSELoss()  # try others: r squared metric scale from -1 (opposite) to 1 (ideal) to infinite (wrong again); accuracy error\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    val_mse_loss = []\n",
    "    val_r2loss = []\n",
    "    val_real_price_percentage_loss = []\n",
    "    percentages = []\n",
    "    training_mse_loss = []\n",
    "    training_r2loss = []\n",
    "    training_real_price_percentage_loss = []\n",
    "    x_axis = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.\n",
    "        running_r2loss = 0.\n",
    "        print(f'Starting new batch {epoch+1}/{epochs}')\n",
    "        # check_accuracy(valloader, model, maxi, mini)\n",
    "        for step, (inputs, labels) in enumerate(dataloader):\n",
    "            # calculate r squarred loss\n",
    "            y_pred = model(inputs)\n",
    "            for pred, label in zip(y_pred, labels):\n",
    "                pr = get_real_price(pred, maxi, mini)\n",
    "                la = get_real_price(label, maxi, mini)\n",
    "                percentages.append(abs(pr-la)/la)\n",
    "            l = loss(y_pred, labels)\n",
    "            # msel = mseloss(y_pred, labels)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += l.item()  # l.item()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                x = r2loss(y_pred, labels).item()\n",
    "                if x >= 1:\n",
    "                    print('x >= 1')\n",
    "                running_r2loss += x\n",
    "                model.train()\n",
    "            if (step+1) % 100 == 0: # if (step+1) % 100 == 0:\n",
    "                mse_l, r2_l, percent_l = evaluate_model(model, valloader, maxi, mini, loss, r2loss)\n",
    "                val_mse_loss.append(mse_l)\n",
    "                val_r2loss.append(r2_l)\n",
    "                val_real_price_percentage_loss.append(percent_l)\n",
    "                training_mse_loss.append(running_loss / 100)\n",
    "                training_r2loss.append(running_r2loss / 100)\n",
    "                training_real_price_percentage_loss.append(sum(percentages)/len(percentages)*100) # to static. Why?\n",
    "                x_axis.append(epoch*len(dataloader) + step)\n",
    "                writer.add_scalar('training_mse_loss', running_loss / 100, epoch*len(dataloader) + step)\n",
    "                writer.add_scalar('training_real_price_percentage_loss', sum(percentages)/len(percentages)*100, epoch*len(dataloader) + step)\n",
    "                running_loss = 0.\n",
    "                running_r2loss = 0.\n",
    "                percentages = []\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoint.pth')\n",
    "        # to load:\n",
    "        # loaded_checkpoint =torch.load('checkpoint.pth')\n",
    "    writer.close()\n",
    "    args = [x_axis, val_mse_loss, training_mse_loss, val_r2loss, training_r2loss, val_real_price_percentage_loss, training_real_price_percentage_loss]\n",
    "    print(args)\n",
    "    graph(*args)\n",
    "    file = '2model.pth'\n",
    "    torch.save(model.state_dict(), file)\n",
    "\n",
    "def graph(x_axis, val_mse_loss, training_mse_loss, val_r2loss, training_r2loss, val_real_price_percentage_loss, training_real_price_percentage_loss):\n",
    "    path = os.path.abspath(os.getcwd())\n",
    "    ra = str(random.randint(1,100))\n",
    "    ver = str(2)\n",
    "    now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_mse_loss, 'b') # ? (0)\n",
    "    ax.plot(x_axis, training_mse_loss, 'r')  # 0\n",
    "    print('mse loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}mse_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_r2loss, 'b') # ? (0)\n",
    "    ax.plot(x_axis, training_r2loss, 'r') # ? (0)\n",
    "    print('r2 loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}r2_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, val_real_price_percentage_loss, 'b') # good\n",
    "    ax.plot(x_axis, training_real_price_percentage_loss, 'r') # good\n",
    "    print('real price percentage loss:')\n",
    "    plt.show()\n",
    "    # plt.savefig(f'plots/{ver}perc_ver{ra}_{now}.png') # comment out for jupyter\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def load_model(data, file = '1model.pth'):\n",
    "    valloader = DataLoader(dataset=val_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "    loaded_model = NeuralNetwork(len(data[0]))\n",
    "    loaded_model.load_state_dict(torch.load(file))\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def evaluate_model(model, valloader, maxi, mini, loss, r2loss):\n",
    "    # print('\\n\\nStart evaluating')\n",
    "    with torch.no_grad():\n",
    "        # try using accuracy in addition to loss\n",
    "        model.eval()\n",
    "        percentages = []\n",
    "        avg_mse_loss = []\n",
    "        avg_r2_loss = []\n",
    "        for step, (inputs, labels) in enumerate(valloader):\n",
    "            mistakes = []\n",
    "            y_pred = model(inputs)\n",
    "            for pred, label in zip(y_pred, labels):\n",
    "                pr = get_real_price(pred, maxi, mini)\n",
    "                la = get_real_price(label, maxi, mini)\n",
    "                # print(f'Estimation: {p}; True: {la}')\n",
    "                mistakes.append(abs(pr-la))\n",
    "                percentages.append(abs(pr-la)/la)\n",
    "            l = loss(y_pred, labels)\n",
    "            r2l = r2loss(y_pred, labels)\n",
    "            # print(f'Average real-price error for this batch was: \\t\\t\\t\\t\\t{sum(mistakes)/len(mistakes)}.')\n",
    "            # print(f'Average real-price error relative to the price in percent was: '\n",
    "            #       f'\\t{sum(percentages)/len(percentages)*100}%.')\n",
    "            # print(f'Average loss for this batch was \\t\\t\\t\\t\\t\\t\\t\\t{l.item()}\\n')\n",
    "            avg_mse_loss.append(l.item())\n",
    "            avg_r2_loss.append(r2l.item())\n",
    "        model.train()\n",
    "        return sum(avg_mse_loss)/len(avg_mse_loss), sum(avg_r2_loss)/len(avg_r2_loss), sum(percentages)/len(percentages)*100\n",
    "\n",
    "    # Graph test over training !\n",
    "    # plot everything on the graph, accuracy, MSEloss, R^2loss, percentage_price%\n",
    "\n",
    "\n",
    "\n",
    "# check accuracy causes Error - not used\n",
    "def check_accuracy(loader, model, maxi, mini):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        aver = []\n",
    "        for x, y in loader:\n",
    "            correct = get_real_price(y.item(), maxi, mini)\n",
    "            resp = model(x)\n",
    "            price = get_real_price(resp.item(), maxi, mini)\n",
    "            aver.append(abs(correct - price))\n",
    "        model.train()\n",
    "        print(sum(aver)/len(aver))\n",
    "        return sum(aver)/len(aver)\n",
    "            #scores = model(x)\n",
    "            #res = scores.unsqueeze(1) - y\n",
    "            #a = torch.mean(res).item()\n",
    "            #aver.append(a)\n",
    "\n",
    "            #_, predictions = scores.max(1)\n",
    "            #num_correct += (predictions == y).sum()\n",
    "            #num_samples += predictions.size(0)\n",
    "\n",
    "        # print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "\n",
    "\n",
    "# both robertas fully copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "def short_roberta(sentences):\n",
    "    # sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = f.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def get_real_price(val, maxi, mini):\n",
    "    x = (maxi-mini) * val + mini\n",
    "    return math.exp(x)\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    # max / min normalization the dataset\n",
    "    # scalars stay normal columns, categories get different columns - one hot encoding\n",
    "    # price on a logarythmic scale\n",
    "\n",
    "    '''\n",
    "    let's build a tensor with the following dimensions:\n",
    "        carat\n",
    "        *cut* (hot encoding)\n",
    "            ideal\n",
    "            premium\n",
    "            good\n",
    "            very good\n",
    "            fair\n",
    "        colour (auch hot encoding)\n",
    "        clarity (dito)\n",
    "        depth\n",
    "        table\n",
    "        price\n",
    "        x\n",
    "        y\n",
    "        z\n",
    "    then min max the full thing.\n",
    "    '''\n",
    "    def onehot():\n",
    "        nb_classes = 6\n",
    "        arr = np.array([[2, 3, 4, 0]])\n",
    "        targets = arr.reshape(-1)\n",
    "        one_hot_targets = np.eye(nb_classes)[targets]\n",
    "        return one_hot_targets\n",
    "\n",
    "    onehot()\n",
    "    np_data = data.to_numpy()\n",
    "\n",
    "    cut_index = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\n",
    "    colour_index = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D':6}\n",
    "    # clarity: (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best))\n",
    "    clarity_index = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1':6, 'IF': 7}\n",
    "    indeces = [cut_index, colour_index, clarity_index]\n",
    "\n",
    "\n",
    "    # carat, cut (5), colour (7), clarity (8), depth, table, price, x, y\n",
    "    new_array = []\n",
    "    prices = []\n",
    "    for i, diamond in enumerate(np_data):\n",
    "        diamond = diamond[1:]\n",
    "        new_diamond = [diamond[0]]\n",
    "        for j in range(3):\n",
    "            index = indeces[j][diamond[j+1]]\n",
    "            zeros = [0.]*len(indeces[j].keys())\n",
    "            zeros[index] = 1.\n",
    "            for k in zeros:\n",
    "                new_diamond.append(k)\n",
    "        for j in [4, 5, 7, 8]:\n",
    "            new_diamond.append(diamond[j])\n",
    "        new_array.append(new_diamond)\n",
    "        prices.append(math.log(diamond[6]))\n",
    "\n",
    "    maxi = max(prices)\n",
    "    mini = min(prices)\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(new_array)\n",
    "    prices = pd.DataFrame(prices)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    return normalized_data, normalized_prices, maxi, mini\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kwargs = {\n",
    "        'epochs':10,\n",
    "        'learning_rate':0.01,\n",
    "        'test_size':1000, # 1000\n",
    "        'train_batch_size':10,\n",
    "        'validation_batch_size':512,\n",
    "        'num_workers':2,\n",
    "        'loss':'nn.MSELoss()',\n",
    "        'optimizer':'torch.optim.SGD(model.parameters(), lr=learning_rate)',\n",
    "        'data_factor': 1\n",
    "    }\n",
    "    main(**kwargs)\n",
    "    #args = [[29, 59, 89], [0.04725663047283888, 0.04288289994001389, 0.03785799648612738], [0.03140254817903042, 0.01449822638183832, 0.013357452619820832], [0.21161172389984131, 0.3071813404560089, 0.3442371547222137], [-0.1830847430229187, 0.05015255331993103, 0.08432324945926667], [86.34664962859036, 78.04877220648744, 74.22272207896643], [67.15759899285841, 91.4195255019661, 84.78499436740307]]\n",
    "    #graph(*args)\n",
    "\n",
    "    # for jupyter:\n",
    "    #   comment out saving of graph (and not model?).\n",
    "    #   change reading of csv\n",
    "    #   adjust start command from jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
